{"cells":[{"cell_type":"markdown","metadata":{"id":"21s8Gqd-D4Sq"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rkR25MqJI_PC"},"source":["# 레이어 2개, relu, size = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4642206,"status":"ok","timestamp":1683375040977,"user":{"displayName":"박민희","userId":"02008349284485949886"},"user_tz":-540},"id":"pWFGORkxJDkL","outputId":"f75d113c-847e-4715-c40b-6bf4f1c28659"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss 0.2218\n","Epoch 10, Loss 0.0004\n","Epoch 20, Loss 0.0045\n","Epoch 30, Loss 0.0021\n","Epoch 40, Loss 0.0050\n","Test Accuracy: 0.9076\n"]}],"source":["import numpy as np\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","class MLP:\n","    def __init__(self, n_input, n_hidden1, n_hidden2, n_output, size):\n","        self.n_input = n_input\n","        self.n_hidden1 = n_hidden1\n","        self.n_hidden2 = n_hidden2\n","        self.n_output = n_output\n","        self.size= size\n","        \n","        # 가중치와 편향 초기화\n","        #self.w1 = np.random.uniform(low =-1.0, high = 1.0, size=(self.n_input, self.n_hidden1))\n","        self.w1 = np.random.randn(self.n_input, self.n_hidden1)/np.sqrt(self.n_input/2)\n","        self.b1 = np.zeros((1, self.n_hidden1))\n","        #self.w2 = np.random.uniform(low =-1.0, high = 1.0, size=(self.n_hidden1, self.n_hidden2))\n","        self.w2 = np.random.randn(self.n_hidden1, self.n_hidden2 )/np.sqrt(self.n_hidden1/2)\n","        self.b2 = np.zeros((1, self.n_hidden2))\n","        #self.w3 = np.random.uniform(low =-1.0, high = 1.0, size=(self.n_hidden2, self.n_output))\n","        self.w3 = np.random.randn(self.n_hidden2, self.n_output )/np.sqrt(self.n_hidden2/2)\n","        self.b3 = np.zeros((1, self.n_output))\n","\n","    def ReLU(self, z):\n","          return np.maximum(0, z)\n","\n","    def softmax(self, z):\n","          z_max = np.max(z)\n","          exp_z = np.exp(z-z_max)\n","          y = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n","          #print(y)\n","          return y\n","\n","    def feed(self, x, w, b):\n","          f = np.dot(x, w) + b\n","          return f\n","    \n","    def cross_entropy(self, y, o):\n","          loss = -np.mean(y * np.log(o)) #cross entropy\n","          #loss = np.square(np.subtract(o_1, y)).mean()\n","          return loss\n","\n","    def feedforward(self, X, y):\n","\n","          input_x = self.feed(X, self.w1, self.b1)\n","          input_x = self.ReLU(input_x)\n","\n","          zsum = self.feed(input_x, self.w2, self.b2)\n","          z_1 = self.ReLU(zsum)\n","\n","          osum = self.feed(z_1, self.w3, self.b3)\n","          o_1 = self.softmax(osum)\n","                  \n","          return o_1, input_x, z_1\n","\n","    def batch (self, X, o, loss, w):\n","\n","          update_w = np.dot(o.T,loss) / self.size #이전 레이어에 전달하여 기울기를 계산\n","          update_b = np.sum(loss, axis=0, keepdims=True) / self.size #(o_1 - y)에서 샘플에 대한 오차를 모두 더한다.\n","          pass_loss = np.dot(loss, w.T) #출력층에서 역전파되는 오차를 이전 은닉층에 전달하기 위한 값\n","          pass_loss[o <= 0] = 0                 #이전 은닉층에서의 가중치 업데이트에 사용\n","          pass_loss = pass_loss * (o > 0) #활성화(relu) 함수 미분과 곱하여 두번째 은닉층에서의 오차값을 구함\n","\n","          return update_w, update_b, pass_loss\n","\n","    def backpropagation(self, X, y, epochs, lr):\n","\n","        batches =X_train.shape[0] // self.size\n","\n","        for epoch in range(epochs):\n","\n","            #섞어서 하나 뽑기\n","            shuffle = np.random.permutation(len(X))\n","            X = X[shuffle]\n","            y = y[shuffle]\n","\n","            for batch in range(batches) :\n","              start = batch * self.size\n","              end = (batch+1) * self.size\n","\n","              X_batch = X[start:end]\n","              y_batch = y[start:end]\n","\n","              o_1, input_x, z_1 = self.feedforward(X_batch,y_batch) #feedfoward한 값을 받아온다.\n","              loss = self.cross_entropy(y_batch, o_1)\n","              sgd_loss = (o_1 - y_batch)\n","\n","              update_w3, update_b3, pass_loss2 =self.batch( X_batch, z_1, sgd_loss, self.w3)\n","              update_w2, update_b2, pass_loss =self.batch(X_batch, input_x, pass_loss2, self.w2)\n","              update_w1 = np.dot(X_batch.T, pass_loss) / self.size\n","              update_b1 = np.sum(pass_loss, axis=0, keepdims=True) / self.size\n","\n","              # 가중치와 편향 업데이트\n","              self.w3 -= lr * update_w3\n","              self.b3 -= lr * update_b3\n","              self.w2 -= lr * update_w2\n","              self.b2 -= lr * update_b2\n","              self.w1 -= lr * update_w1\n","              self.b1 -= lr * update_b1\n","            \n","            \n","           # 손실 출력\n","            if epoch % 10 == 0:\n","                print(f'Epoch {epoch}, Loss {loss:.4f}') \n","\n","          \n","    def predict(self, X, y):\n","          o_1, input_x, z_1= self.feedforward(X, y)\n","          y_pred = np.argmax(o_1, axis=1)\n","          return y_pred\n","\n","mnist = fetch_openml('mnist_784', cache=False)\n","mnist.data.shape\n","\n","X, y = mnist[\"data\"], mnist[\"target\"]\n","\n","\n","# X 데이터 정규화\n","minmaxscalar = MinMaxScaler()\n","X = minmaxscalar.fit_transform(X)\n","#X = (X - X.mean(axis=0)) / X.std(axis=0)\n","\n","#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","# 레이블을 one-hot encoding으로 변환\n","lb = LabelBinarizer()\n","y_train = lb.fit_transform(y_train)\n","y_test = lb.transform(y_test)\n","X_train = X_train.reshape(-1, 784) / 255.0\n","X_test = X_test.reshape(-1, 784) / 255.0\n","\n","# MLP 모델 생성\n","mlp = MLP(n_input=X_train.shape[1], n_hidden1=256, n_hidden2= 128, n_output=10, size=1)\n","\n","# 모델 학습\n","mlp.backpropagation(X_train, y_train, epochs=50, lr=0.01)\n","\n","# 모델 테스트\n","y_pred = mlp.predict(X_test, y_test)\n","\n","# 정확도 계산\n","y_test_true = np.argmax(y_test, axis=1)\n","accuracy = np.mean(y_pred == y_test_true)\n","print(f'Test Accuracy: {accuracy:.4f}')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPYYF6wPSaHEgzIfJZgxidE"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}